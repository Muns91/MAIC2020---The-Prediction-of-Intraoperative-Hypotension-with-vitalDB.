{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ Load 파일  ##########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "tf.random.set_seed(777)\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import argmax\n",
    "import os\n",
    "from sklearn.metrics import average_precision_score\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "from pandas import DataFrame\n",
    "import random\n",
    "import csv\n",
    "from tensorflow import keras\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from keras.layers import Concatenate\n",
    "\n",
    "N_batch = 10000 # batch_size\n",
    "N_epochs = 5 # Epoch length\n",
    "N_avg = 5 # ex) N개마다 평균 -> 2000/5 => 400\n",
    "Input_size = 405"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################### 모델 불러오기 함수 ######################################################\n",
    "def load_model():\n",
    "    model.load_weights(f\"checkpoint/cp-{1}.ckpt\")\n",
    "################################################### AUPRC 함수  ####################################################    \n",
    "def Metric_Aprecision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision  \n",
    "\n",
    "def get_data(percent, Training_zero, Training_one, Test_data_X, num_zero, num_one):\n",
    "    print(\"데이터 처리중...\")\n",
    "    #1.OO, 2.OC, 3.CO, 4.CC\n",
    "\n",
    "    train_zero_data_origin = Training_zero\n",
    "    train_one_data_origin = Training_one\n",
    "    test_data_origin = Test_data_X\n",
    "    \n",
    "    data_percent = int(100 / percent) #100에 딱 떨어지는 몫이 되어야 가능함\n",
    "    \n",
    "    # get split length\n",
    "    zero_val_len = int(len(train_zero_data_origin) / data_percent) \n",
    "    one_val_len = int(len(train_one_data_origin) / data_percent)\n",
    "    # print(zero_val_len, one_val_len, len(train_zero_data_origin), len(train_one_data_origin))\n",
    "\n",
    "    train_zero_data = np.array(train_zero_data_origin)\n",
    "    train_one_data = np.array(train_one_data_origin)\n",
    "\n",
    "    for i in range(data_percent):\n",
    "        zero_val = []\n",
    "        one_val = []\n",
    "        for j in range(data_percent):\n",
    "            zero_val.append(train_zero_data[zero_val_len*j:zero_val_len*(j+1), :])\n",
    "            one_val.append(train_one_data[one_val_len*j:one_val_len*(j+1), :])\n",
    "    \n",
    "    ## validation data\n",
    "    zero_val_data = zero_val[num_zero]\n",
    "    one_val_data = one_val[num_one]\n",
    "    \n",
    "    ## train data set(except validation data)\n",
    "    train_zero = zero_val\n",
    "    del train_zero[num_zero]  # used valdation set delete from train_data_set\n",
    "\n",
    "    train_one = one_val\n",
    "    del train_one[num_one] # used valdation set delete from train_data_set\n",
    "    \n",
    "    #list to array\n",
    "    train_zero_origin = np.array(train_zero)\n",
    "    train_one_origin = np.array(train_one)\n",
    "    \n",
    "    #2d array -> 1d array\n",
    "    train_zero = [element for array in train_zero_origin for element in array]\n",
    "    train_one =[element for array in train_one_origin for element in array]\n",
    "    \n",
    "    ## train_set\n",
    "    train_zero = np.array(train_zero)\n",
    "    train_one = np.array(train_one)\n",
    "    train_set = np.vstack((train_zero, train_one))\n",
    "\n",
    "    ## validation set\n",
    "    zero_val_data = np.array(zero_val_data)\n",
    "    one_val_data = np.array(one_val_data)\n",
    "    val_set = np.vstack((zero_val_data, one_val_data))\n",
    "    \n",
    "    def shuffle_data(zero_data, one_data):    \n",
    "        # parameters\n",
    "        N = len(zero_data)\n",
    "        K = len(one_data)\n",
    "        L = int(np.ceil(N / K))\n",
    "        Q = N // L\n",
    "        R = N % L\n",
    "        U = K - Q\n",
    "\n",
    "        total_seq = []\n",
    "        num = 0\n",
    "        for i in range(Q):\n",
    "            if i > Q-U:\n",
    "                zero_seq = zero_data[L*i:L*(i+1),:]\n",
    "                one_seq = one_data[Q-U+(2*num):Q-U+(2*(num+1)) ,:]\n",
    "                shuffle_seq = np.vstack((zero_seq, one_seq))          \n",
    "                np.random.shuffle(shuffle_seq)\n",
    "                total_seq.append(shuffle_seq) \n",
    "                num = num + 1\n",
    "            else:\n",
    "                zero_seq = zero_data[L*i:L*(i+1),:]\n",
    "                one_seq = one_data[i, :]\n",
    "                shuffle_seq = np.vstack((zero_seq, one_seq))          \n",
    "                np.random.shuffle(shuffle_seq)\n",
    "                total_seq.append(shuffle_seq)  \n",
    "            if i == Q-1:\n",
    "                total_seq.append((zero_data[L*(i+1):,:]))\n",
    "        \n",
    "#         print(\"shuffle 데이터 확인 : \", total_seq[Q-1])\n",
    "        total_seq = np.array(total_seq)\n",
    "        # 2d list to 1d list and convert 2d array likely (none, 404)\n",
    "        total_seq = [element for array in total_seq for element in array]\n",
    "        total_seq = np.array(total_seq)\n",
    "        return total_seq\n",
    "    \n",
    "    #train_set\n",
    "    train_set = shuffle_data(train_zero, train_one)\n",
    "    x_data = train_set[:, :-1]\n",
    "    y_data = train_set[:, -1]\n",
    "\n",
    "    # validation_set(divide label)\n",
    "    val_set = shuffle_data(zero_val_data, one_val_data)\n",
    "    val_x_data = val_set[:, :-1]\n",
    "    val_y_data = val_set[:, [-1]]\n",
    "    \n",
    "    input_shape = len(x_data[0])\n",
    "    x_data = x_data.reshape(x_data.shape[0], input_shape, 1)\n",
    "    val_x_data = val_x_data.reshape(val_x_data.shape[0], input_shape, 1)\n",
    "    \n",
    "    test_data = np.array(test_data_origin)\n",
    "    test_data = test_data.reshape(test_data.shape[0], input_shape, 1)\n",
    "    \n",
    "#     print(\"x_data의 shape 확인 : \", x_data.shape)\n",
    "#     print(\"val_x_data의 shape 확인 : \", val_x_data.shape)\n",
    "#     print(\"test_data의 shape 확인 : \", test_data.shape)\n",
    "    \n",
    "    print(\"데이터 처리 완료\")\n",
    "    return x_data, y_data, val_x_data, val_y_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_zero = pd.read_csv('zero_data.csv', header=None)\n",
    "Training_one = pd.read_csv('one_data.csv', header=None)\n",
    "Test_data_X = pd.read_csv('test_data.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"Best_model/cp-{epoch:d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1,period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 처리중...\n",
      "데이터 처리 완료\n"
     ]
    }
   ],
   "source": [
    "x_data, y_data, val_x_data, val_y_data, test_data = get_data(10, Training_zero, Training_one, Test_data_X, 0, 0)\n",
    "Learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 202, 60)           660       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 202, 60)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 101, 50)           15050     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 101, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 51, 50)            12550     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 51, 50)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 26, 50)            12550     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 26, 50)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 13, 50)            12550     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 13, 50)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 7, 50)             12550     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 7, 50)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 4, 50)             12550     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 4, 50)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 2, 50)             12550     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 2, 50)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2, 8)              408       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 91,435\n",
      "Trainable params: 91,435\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "################################################ CNN 1D ###########################################################\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(input_shape=(x_data.shape[1],1), kernel_size=10, filters=60, strides=2, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool1D(pool_size=1),\n",
    "    tf.keras.layers.Conv1D(kernel_size=5, filters=50, strides=2, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool1D(pool_size=1),\n",
    "    tf.keras.layers.Conv1D(kernel_size=5, filters=50, strides=2, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool1D(pool_size=1),\n",
    "    tf.keras.layers.Conv1D(kernel_size=5, filters=50, strides=2, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool1D(pool_size=1),\n",
    "    tf.keras.layers.Conv1D(kernel_size=5, filters=50, strides=2, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool1D(pool_size=1),\n",
    "    tf.keras.layers.Conv1D(kernel_size=5, filters=50, strides=2, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool1D(pool_size=1),\n",
    "    tf.keras.layers.Conv1D(kernel_size=5, filters=50, strides=2, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool1D(pool_size=1),\n",
    "    tf.keras.layers.Conv1D(kernel_size=5, filters=50, strides=2, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool1D(pool_size=1),\n",
    "    tf.keras.layers.Dense(units=8, activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=Learning_rate), loss='binary_crossentropy',metrics=['accuracy',Metric_Aprecision])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 514619 samples, validate on 57179 samples\n",
      "510000/514619 [============================>.] - ETA: 0s - loss: 0.1767 - accuracy: 0.9350 - Metric_Aprecision: 0.6554\n",
      "Epoch 00001: saving model to Best_model/cp-1.ckpt\n",
      "514619/514619 [==============================] - 15s 28us/sample - loss: 0.1768 - accuracy: 0.9350 - Metric_Aprecision: 0.6556 - val_loss: 0.1820 - val_accuracy: 0.9358 - val_Metric_Aprecision: 0.6560\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(f\"checkpoint/cp-{1}.ckpt\")\n",
    "N_batch = 10000\n",
    "N_epochs = 1\n",
    "out_put1 = model.fit(x_data, y_data, batch_size = N_batch, validation_data = (val_x_data, val_y_data), callbacks=[cp_callback], epochs=N_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train SET AUPRC score :  0.5225811318824329\n",
      "Val SET AUPRC score :  0.5222554273613098\n"
     ]
    }
   ],
   "source": [
    "#train set AUPRC\n",
    "pred = model.predict(x_data, batch_size = N_batch)\n",
    "val_pred  = model.predict(val_x_data, batch_size = N_batch)\n",
    "AUPRC = average_precision_score(y_data, pred)\n",
    "AUPRC_val = average_precision_score(val_y_data, val_pred)\n",
    "print('Train SET AUPRC score : ', AUPRC)\n",
    "print('Val SET AUPRC score : ', AUPRC_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set AUPRC\n",
    "test_output = model.predict(test_data, batch_size = N_batch)\n",
    "np.savetxt('Validation_output.csv', test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x208a7ec76c8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(f\"checkpoint/cp-{1}.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train SET AUPRC score :  0.519791637100393\n",
      "Val SET AUPRC score :  0.5240595537800274\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_data, batch_size = N_batch)\n",
    "val_pred  = model.predict(val_x_data, batch_size = N_batch)\n",
    "AUPRC = average_precision_score(y_data, pred)\n",
    "AUPRC_val = average_precision_score(val_y_data, val_pred)\n",
    "print('Train SET AUPRC score : ', AUPRC)\n",
    "print('Val SET AUPRC score : ', AUPRC_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = model.predict(test_data, batch_size = N_batch)\n",
    "np.savetxt('Final_Report_output.csv', test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
